1.
CloudWatch
Within the CloudWatch page, I would go to the Application monitoring section, and I would look in the Synthetics Canaries 
section to identify the test that is failing and open the details. I would look at the failed canary runs and the success 
percent graphs to see if this issue is constant or intermittent. I would also look at screenshots and logs of the failed 
run(s). If all tests are failing, I would look to see if a recent deployment is the root cause. I would request to have 
the deployment rolled back to see if that resolves the issue. If the issue is intermittent, I would then move to Splunk 
to see if I can identify the instance(s) that might be having issues.

Splunk
In Splunk, I would run queries against the index that is indexing the logs for the Web Applications and look for a possible 
host that is throwing the errors. I would look at fields like Host and Message to get more information. I would also look 
for the status code(503, 404) that is being thrown by the instance(s). 

APM(Dynatrace)
In an APM I would be able to see the entire service flow and all the endpoints that are being hit during these calls, both 
prior and after this service is called. This would help me identify if there is a different application/service that is the 
actual root cause. I would also be able to see trace details that would also help in identifying a possible root cause.

2.
You would need to have service level indicators like availability, response time and error rate in place and thresholds set 
to be able to confirm that your application is working as expected. Having an APM that helps you set those baselines will 
allow you to identify an issue much faster. 

3.
Better testing in the lower environments would help a lot in reducing these types of outages. Performance testing, finding 
ways to have more redundancy and also utilizing microservices would be possible improvements to a platform.


Postmortem summary
Postmortem owner	
Incident	 Web Application - Site Down
Priority	p0 / p1 / p2+
Affected services	 
	â€¢	Web Application - Entire Site
  
Postmortem report

Leadup
List the sequence of events that led to the incident.

Fault
Describe how the change that was implemented didn't work as expected. If available, include relevant data visualizations.	 
  
Impact
Describe how internal and external users were impacted during the incident. Include how many support cases were raised.	 
  
Detection
Report when the team detected the incident and how they knew it was happening. Describe how the team could've improved time to detection.	 
  
Response
Report who responded to the incident and describe what they did at what times. Include any delays or obstacles to responding.	 
  
Recovery
Report how the user impact was mitigated and when the incident was deemed resolved. Describe how the team could've improved time to mitigation.	 

Timeline
Detail the incident timeline using UTC to standardize for timezones. Include lead-up events, post-impact event, and any decisions or changes made.	 
  
Blameless root cause
Note the final root cause and describe what needs to change without placing blame to prevent this class of incident from recurring.	 
  
Backlog check
Review the engineering backlog to find out if there was unplanned work that could've prevented the incident or reduced its impact.	 
  
Related incidents
Check if any past incidents could've had the same root cause. Note what mitigation was attempted in those incidents and ask why this incident occurred again.	 

Lessons learned
Describe what you learned, what went well, and how you can improve.	 

Follow-up tasks
List the Jira issues created to prevent this class of incident in the future. Note who is responsible, when they have to complete the work, and where that work is being tracked.	 

